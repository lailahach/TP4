{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be66742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import confusion_matrix ,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd93caec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28182 samples with 33 unique classes.\n",
      " Train : 16908 samples , Validation : 5637 samples , Test : 5637 samples \n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(),'images-data-64/tifinagh-images/')\n",
    "#print(data_dir)\n",
    "current_working_directory = os.getcwd ()\n",
    "#print(current_working_directory )\n",
    "\n",
    "# Charger le fichier CSV contenant les etiquettes\n",
    "try:\n",
    "    labels_df = pd.read_csv(os.path.join(os.getcwd(),'images-data-64/labels-map.csv'))\n",
    "    \n",
    "    assert 'image_path' in labels_df.columns and 'label' in labels_df.columns, \"CSV must contain 'image_path' and 'label' columns\"\n",
    "except FileNotFoundError : \n",
    "    print (\"labels-map.csv not found. Please check the dataset structure.\")\n",
    "\n",
    "    # Alternative : construire un DataFrame partir des dossiers\n",
    "                        \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for label_dir in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label_dir)\n",
    "        if os.path.isdir(label_path):\n",
    "            for img_name in os.listdir(label_path):\n",
    "                image_paths.append(os.path.join(label_path, img_name))\n",
    "                labels.append(label_dir)\n",
    "\n",
    "    # Créer le DataFrame\n",
    "    labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
    "\n",
    "# Vérifier le DataFrame\n",
    "assert not labels_df.empty, \"No data loaded. Check dataset files.\"\n",
    "print(f\"Loaded {len(labels_df)} samples with {labels_df['label'].nunique()} unique classes.\")\n",
    "\n",
    "# Encoder les étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Fonction pour charger et prétraiter une image\n",
    "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image: convert to grayscale, resize, normalize\n",
    "    \"\"\"\n",
    "    assert os.path.exists(image_path), f\"Image not found: {image_path}\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    assert img is not None, f\"Failed to load image: {image_path}\"\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype(np.float32) / 255.0  # Normalisation\n",
    "    return img.flatten()  # Aplatir pour le réseau de neurones\n",
    "\n",
    "# Charger toutes les images\n",
    "X = np.array([load_and_preprocess_image(path) for path in labels_df['image_path']])\n",
    "y = labels_df['label_encoded'].values\n",
    "# Vérifier les dimensions\n",
    "assert X.shape [0] == y.shape[0] , \" Mismatch between number of images and labels \"\n",
    "assert X.shape [1] == 32 * 32 , f\" Expected flattened image size of {32*32} , got {X.shape [1]} \"\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "y_one_hot = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y_one_hot, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=np.argmax(y_temp, axis=1), random_state=42)\n",
    "\n",
    "# Diviser en ensembles d'entrainement , validation et test\n",
    "#X_temp , X_test , y_temp , y_test = train_test_split (X , y , test_size =0.2 , stratify =y , random_state =42)\n",
    "#X_train , X_val , y_train , y_val = train_test_split ( X_temp , y_temp , test_size =0.25 , stratify = y_temp , random_state =42)\n",
    "\n",
    "# Convertir explicitement en NumPy arrays\n",
    "X_train = np.array ( X_train )\n",
    "X_val = np.array ( X_val )\n",
    "X_test = np.array ( X_test )\n",
    "y_train = np.array ( y_train )\n",
    "y_val = np.array ( y_val )\n",
    "y_test = np.array (y_test )\n",
    "\n",
    "assert X_train.shape [0] + X_val.shape [0] + X_test.shape [0] == X.shape [0] , \"Train-val-test split sizes must sum to total samples\"\n",
    "print (f\" Train : { X_train.shape [0]} samples , Validation : { X_val.shape [0]} samples , Test : { X_test.shape [0]} samples \")\n",
    "\n",
    "# Encoder les étiquettes en one-hot pour la classification multiclasse\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_one_hot = np.array(one_hot_encoder.fit_transform(y_train.reshape(-1, 1)))\n",
    "y_val_one_hot = np.array(one_hot_encoder.transform(y_val.reshape(-1, 1)))\n",
    "y_test_one_hot = np.array(one_hot_encoder.transform(y_test.reshape(-1, 1)))\n",
    "\n",
    "\n",
    "# Vérifier que les tableaux one-hot sont bien des tableaux NumPy\n",
    "assert isinstance(y_train_one_hot, np.ndarray), \"y_train_one_hot must be a numpy array\"\n",
    "assert isinstance(y_val_one_hot, np.ndarray), \"y_val_one_hot must be a numpy array\"\n",
    "assert isinstance(y_test_one_hot, np.ndarray), \"y_test_one_hot must be a numpy array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ae5fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73105858, 0.26894142],\n",
       "       [0.26894142, 0.73105858]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation: max(0, x)\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU must be a numpy array\"\n",
    "    result = np.maximum(0, x)\n",
    "    assert np.all(result >= 0), \"ReLU output must be non-negative\"\n",
    "    return result\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU: 1 if x > 0, else 0\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU derivative must be a numpy array\"\n",
    "    result = (x > 0).astype(int)\n",
    "    assert np.all((result == 0) | (result == 1)), \"ReLU derivative must be 0 or 1\"\n",
    "    return result\n",
    "\n",
    "def Softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax activation : exp (x) / sum ( exp (x))\n",
    "    \"\"\"\n",
    "    assert isinstance (x , np.ndarray ) , \" Input to softmax must be a numpy array\"\n",
    "    exp_x = np.exp(x)\n",
    "    result = exp_x / np.sum(exp_x, axis = 1, keepdims = True)\n",
    "    assert np.all((result >= 0) & (result <= 1)),\"Softmax output must be in [0,1]\"\n",
    "    assert np.allclose(np.sum(result, axis =1) , 1) , \" Softmax output must sum to 1 per sample \"\n",
    "    return result\n",
    "Softmax(np.array([[1,0],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a30b91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNeuralNetwork :\n",
    "    \n",
    "    def __init__(self,layer_sizes, learning_rate = 0.01, l2_lambda=0.0, optimizer=\"SGD\"):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with given layer sizes and learning rate .\n",
    "        layer_sizes : List of integers [ input_size , hidden1_size ,... , output_size ]\n",
    "\n",
    "        \"\"\"\n",
    "        assert isinstance(layer_sizes,list) and len (layer_sizes)>= 2 ,\"layer_sizes must be a list with at least 2 elements\"\n",
    "        assert all(isinstance (size,int) and size > 0 for size in layer_sizes),\"All layer sizes must be positive integers\"\n",
    "        assert isinstance(learning_rate,(int,float)) and learning_rate > 0, \" Learning rate must be a positive number \"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda=l2_lambda\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.optimizer = optimizer\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            w = np.random.randn(self.layer_sizes[i],self.layer_sizes[i+1]) * 0.01\n",
    "            b = np.random.randn(1,self.layer_sizes[i+1])\n",
    "            assert w.shape == (layer_sizes[i],layer_sizes[i+1]),f\"Weight matrix {i+1} has incorrect shape\"\n",
    "            assert b.shape == (1,layer_sizes [i+1]),f\"Bias vector{i+1} has incorrect shape \"\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        self.momentum = 0.9\n",
    "        self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation: Z^{[l]} = A^{[l-1]}W^{[l]} + b^{[l]}, A^{[l]} = g(Z^{[l]})\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "\n",
    "        self.z_values = []\n",
    "        self.activations = [X]\n",
    "\n",
    "        # Couches cachées avec ReLU\n",
    "        for i in range(len(self.layer_sizes) - 2):\n",
    "            z = self.activations[i] @ self.weights[i] + self.biases[i]\n",
    "            assert z.shape == (X.shape[0], self.layer_sizes[i+1]), f\"Z^{i+1} has incorrect shape\"\n",
    "            a = relu(z)\n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(a)\n",
    "\n",
    "        # Dernière couche avec Softmax\n",
    "        z = self.activations[-1] @ self.weights[-1] + self.biases[-1]\n",
    "        assert z.shape == (X.shape[0], self.layer_sizes[-1]), \"Output Z has incorrect shape\"\n",
    "        output = Softmax(z)\n",
    "        assert output.shape == (X.shape[0], self.layer_sizes[-1]), \"Output A has incorrect shape\"\n",
    "        self.z_values.append(z)\n",
    "        self.activations.append(output)\n",
    "\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Categorical Cross-Entropy: J = -1/m * sum(y_true * log(y_pred))\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to loss must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        assert not np.isnan(loss), \"Loss computation resulted in NaN\"\n",
    "        return loss\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute accuracy: proportion of correct predictions\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to accuracy must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "        assert 0 <= accuracy <= 1, \"Accuracy must be between 0 and 1\"\n",
    "        #print(\"accuracy = \",accuracy)\n",
    "        return accuracy\n",
    "\n",
    "    def backward(self,X,y,outputs):\n",
    "        \"\"\"\n",
    "        Backpropagation: compute dW^{[l]}, db^{[l]} for each layer\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray) and isinstance(outputs, np.ndarray), \"Inputs to backward must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape == outputs.shape, \"y and outputs must have the same shape\"\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        self.d_weights = [None] * len(self.weights)\n",
    "        self.d_biases = [None] * len(self.biases)\n",
    "        \n",
    "        dZ = outputs - y\n",
    "        assert dZ.shape == outputs.shape , \"dZ for output layer has incorrect shape\"\n",
    "        self.d_weights[-1] = (self.activations [-2].T @ dZ) / m\n",
    "        self.d_biases[-1] = np.sum(dZ ,axis=0 , keepdims = True) / m\n",
    "        \n",
    "        for l in range(len(self.weights)-2,-1,-1 ):\n",
    "            dZ = (dZ @ self.weights[l+1].T) * relu_derivative(self.z_values[l])\n",
    "            assert dZ.shape == (X.shape [0] , self.layer_sizes[l +1]), f\"dZ ^{[ i +1]} has incorrect shape\"\n",
    "            self.d_weights[l] = (self.activations [l].T @ dZ) / m\n",
    "            self.d_biases[l] = np.sum(dZ ,axis=0 , keepdims = True) / m   \n",
    "            \n",
    "        # Optimisation\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-8\n",
    "        for i in range(len(self.weights)):\n",
    "\n",
    "            # Ajout régularisation L2\n",
    "            self.d_weights[i] += (self.l2_lambda * self.weights[i]) / m\n",
    "\n",
    "            if self.optimizer == \"momentum\":\n",
    "                self.v_weights[i] = self.momentum * self.v_weights[i] - self.learning_rate * self.d_weights[i]\n",
    "                self.v_biases[i] = self.momentum * self.v_biases[i] - self.learning_rate * self.d_biases[i]\n",
    "\n",
    "            elif self.optimizer == \"rmsprop\":\n",
    "                if not hasattr(self, 's_weights'):\n",
    "                    self.s_weights = [np.zeros_like(w) for w in self.weights]\n",
    "                    self.s_biases = [np.zeros_like(b) for b in self.biases]\n",
    "                self.s_weights[i] = beta1 * self.s_weights[i] + (1 - beta1) * self.d_weights[i]**2\n",
    "                self.s_biases[i] = beta1 * self.s_biases[i] + (1 - beta1) * self.d_biases[i]**2\n",
    "                self.v_weights[i] = -self.learning_rate * self.d_weights[i] / (np.sqrt(self.s_weights[i]) + epsilon)\n",
    "                self.v_biases[i] = -self.learning_rate * self.d_biases[i] / (np.sqrt(self.s_biases[i]) + epsilon)\n",
    "\n",
    "            elif self.optimizer == \"adam\":\n",
    "                if not hasattr(self, 'm_weights'):\n",
    "                    self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "                    self.vw_weights = [np.zeros_like(w) for w in self.weights]\n",
    "                    self.m_biases = [np.zeros_like(b) for b in self.biases]\n",
    "                    self.vw_biases = [np.zeros_like(b) for b in self.biases]\n",
    "                    self.t = 0\n",
    "                self.t += 1\n",
    "                self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * self.d_weights[i]\n",
    "                self.vw_weights[i] = beta2 * self.vw_weights[i] + (1 - beta2) * (self.d_weights[i] ** 2)\n",
    "                m_corr = self.m_weights[i] / (1 - beta1 ** self.t)\n",
    "                v_corr = self.vw_weights[i] / (1 - beta2 ** self.t)\n",
    "                self.v_weights[i] = -self.learning_rate * m_corr / (np.sqrt(v_corr) + epsilon)\n",
    "\n",
    "                self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * self.d_biases[i]\n",
    "                self.vw_biases[i] = beta2 * self.vw_biases[i] + (1 - beta2) * (self.d_biases[i] ** 2)\n",
    "                mb_corr = self.m_biases[i] / (1 - beta1 ** self.t)\n",
    "                vb_corr = self.vw_biases[i] / (1 - beta2 ** self.t)\n",
    "                self.v_biases[i] = -self.learning_rate * mb_corr / (np.sqrt(vb_corr) + epsilon)\n",
    "\n",
    "            else:  # SGD par défaut\n",
    "                self.v_weights[i] = -self.learning_rate * self.d_weights[i]\n",
    "                self.v_biases[i] = -self.learning_rate * self.d_biases[i]\n",
    "\n",
    "            # Mise à jour finale des poids\n",
    "            self.weights[i] += self.v_weights[i]\n",
    "            self.biases[i] += self.v_biases[i]         \n",
    "            \n",
    "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch SGD, with validation\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n",
    "        assert isinstance(X_val, np.ndarray) and isinstance(y_val, np.ndarray), \"X_val and y_val must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape[1] == self.layer_sizes[-1], f\"Output dimension ({y.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert X_val.shape[1] == self.layer_sizes[0], f\"Validation input dimension ({X_val.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y_val.shape[1] == self.layer_sizes[-1], f\"Validation output dimension ({y_val.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert isinstance(epochs, int) and epochs > 0, \"Epochs must be a positive integer\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \"Batch size must be a positive integer\"\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(X.shape[0]) # indice aleatoire represente une image alétoire = vecteur de meme taille que x_train\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size): # 0 1024 32\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "                outputs = self.forward(X_batch)\n",
    "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
    "                self.backward(X_batch, y_batch, outputs)\n",
    "\n",
    "            # Calculer les pertes et accuracies\n",
    "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
    "            train_pred = self.forward(X)\n",
    "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
    "            val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, val_pred)\n",
    "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\") \n",
    "\n",
    "        return train_losses , val_losses , train_accuracies ,val_accuracies\n",
    "\n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "\n",
    "        outputs = self.forward(X)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        assert predictions.shape == (X.shape[0],), \"Predictions have incorrect shape\"\n",
    "        return predictions\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71cd32",
   "metadata": {},
   "source": [
    "### First Configuration with regularization and optimizer methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c0eb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer et entraîner le modèle\n",
    "layer_sizes = [X_train.shape[1], 64, 32, num_classes]  # 64 et 32 neurones cachés, 33 classes\n",
    "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.01, l2_lambda = 0.01,optimizer=\"adam\")\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
    "#    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set):\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('confusion Matrix (Test set) with Adam optimizer & lr=0.01'  )\n",
    "plt.xlabel('Prédit')\n",
    "plt.ylabel('Réel')\n",
    "plt.savefig('images/confusion_matrix_adam_L2_0_001_lr_0.01_cv.png')\n",
    "plt.close()\n",
    "\n",
    "# Courbes de perte et d’accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Courbe de perte\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_title('Loss curve with Adam optimizer & lr=0.01_cv')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Courbe de précision\n",
    "ax2.plot(train_accuracies, label='Train Accuracy')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "ax2.set_title('Precision curve with Adam optimizer & lr=0.01_cv')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "fig.savefig('images/loss_accuracy_plot_adam_L2_0_01_lr_0.001_cv.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab2733",
   "metadata": {},
   "source": [
    "### Second Configuration with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92727cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate(model_class, X, y_one_hot, k=5, epochs=100, batch_size=32, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée K-fold sur un modèle MultiClassNeuralNetwork.\n",
    "    \n",
    "    Parameters:\n",
    "        model_class: la classe du modèle à instancier (ex: MultiClassNeuralNetwork)\n",
    "        X (np.ndarray): données d'entrée (features)\n",
    "        y_one_hot (np.ndarray): étiquettes one-hot (n_samples, n_classes)\n",
    "        k (int): nombre de folds\n",
    "        epochs (int): nombre d'époques par fold\n",
    "        batch_size (int): taille des mini-lots\n",
    "        model_kwargs: hyperparamètres pour initialiser le modèle\n",
    "    \n",
    "    Returns:\n",
    "        Moyenne des précisions et pertes sur les folds\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"\\nFold {fold+1}/{k}\")\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "\n",
    "        model = model_class(**model_kwargs)\n",
    "        train_loss, val_loss, train_accuracy, val_accuracy = model.train(X_train, y_train, X_val, y_val, epochs, batch_size)\n",
    "\n",
    "        val_accuracies.append(val_accuracy[-1])  # Dernier epoch\n",
    "        val_losses.append(val_loss[-1])\n",
    "        train_accuracies.append(train_accuracy[-1])\n",
    "        train_losses.append(train_loss[-1])\n",
    "    \n",
    "    print(f\"\\nMoyenne des précisions de validation : {np.mean(val_accuracies):.4f}\")\n",
    "    print(f\"Moyenne des pertes de validation : {np.mean(val_losses):.4f}\")\n",
    "    print(f\"\\nMoyenne des précisions d'entrainement : {np.mean(train_accuracies):.4f}\")\n",
    "    print(f\"Moyenne des pertes d'entrainement : {np.mean(train_losses):.4f}\")\n",
    "\n",
    "    return  val_accuracies, val_losses, train_accuracies, train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57be8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "(557964, 2)\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 0, Train Loss: 2.8125, Validation Loss: 2.2071, Train Acc: 0.2889, Val Acc: 0.2906\n",
      "Epoch 20, Train Loss: 0.3812, Validation Loss: 0.5220, Train Acc: 0.8864, Val Acc: 0.8380\n",
      "Epoch 40, Train Loss: 0.2339, Validation Loss: 0.4601, Train Acc: 0.9297, Val Acc: 0.8602\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 0, Train Loss: 2.8041, Validation Loss: 2.2103, Train Acc: 0.2849, Val Acc: 0.2821\n",
      "Epoch 20, Train Loss: 0.3804, Validation Loss: 0.5111, Train Acc: 0.8879, Val Acc: 0.8368\n",
      "Epoch 40, Train Loss: 0.2349, Validation Loss: 0.4235, Train Acc: 0.9351, Val Acc: 0.8680\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 0, Train Loss: 2.8108, Validation Loss: 2.2367, Train Acc: 0.2840, Val Acc: 0.2741\n",
      "Epoch 20, Train Loss: 0.4013, Validation Loss: 0.5617, Train Acc: 0.8825, Val Acc: 0.8197\n",
      "Epoch 40, Train Loss: 0.2509, Validation Loss: 0.5103, Train Acc: 0.9243, Val Acc: 0.8421\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 0, Train Loss: 2.7790, Validation Loss: 2.1299, Train Acc: 0.3051, Val Acc: 0.3032\n",
      "Epoch 20, Train Loss: 0.4152, Validation Loss: 0.5326, Train Acc: 0.8826, Val Acc: 0.8282\n"
     ]
    }
   ],
   "source": [
    "num_classes = y_one_hot.shape[1]\n",
    "print(num_classes)\n",
    "print(y_train_one_hot.shape)\n",
    "model_args = {\n",
    "    'layer_sizes': [X.shape[1], 64, 32, num_classes],\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer': 'adam',\n",
    "    'l2_lambda': 0.01\n",
    "}\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = cross_validate(MultiClassNeuralNetwork, X, y_one_hot, k=5, epochs=50, batch_size=32, **model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes des courbes par époque\n",
    "mean_train_losses = np.mean(train_losses, axis=0)\n",
    "mean_val_losses = np.mean(val_losses, axis=0)\n",
    "mean_train_accs = np.mean(train_accuracies, axis=0)\n",
    "mean_val_accs = np.mean(val_accuracies, axis=0)\n",
    "\n",
    "# Tracer les courbes moyennes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Courbe de perte\n",
    "ax1.plot(mean_train_losses, label='Mean Train Loss')\n",
    "ax1.plot(mean_val_losses, label='Mean Validation Loss')\n",
    "ax1.set_title('Loss Curve with Cross Validation (Adam, lr=0.01)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Courbe de précision\n",
    "ax2.plot(mean_train_accs, label='Mean Train Accuracy')\n",
    "ax2.plot(mean_val_accs, label='Mean Validation Accuracy')\n",
    "ax2.set_title('Accuracy Curve with Cross Validation (Adam, lr=0.01)')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('images/loss_accuracy_plot_adam_L2_0_01_lr_0_01_cv.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243bec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
